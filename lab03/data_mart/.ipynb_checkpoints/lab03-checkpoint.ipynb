{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -OL http://downloads.datastax.com/community/dsc.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json \n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.5.1,org.apache.spark:spark-sql_2.11:2.4.5,org.apache.spark:spark-core_2.11:2.4.5 --conf spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtensions --conf spark.cassandra.connection.host=10.0.0.31:9042 pyspark-shell'\n",
    "\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"postgresql\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-42.4.0.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'an_en'\n",
    "password = 'QjFvQaAt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o91.load.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8eac251aa75e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jdbc\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jdbc:postgresql://10.0.0.31:5432/labdata\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dbtable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"domain_cats\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"password\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"driver\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.postgresql.Driver\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o91.load.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "cats = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://10.0.0.31:5432/labdata\") \\\n",
    "    .option(\"dbtable\", \"domain_cats\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|              domain|category|\n",
      "+--------------------+--------+\n",
      "|national-lottery....|gambling|\n",
      "|          rivalo.com|gambling|\n",
      "|             xoso.me|gambling|\n",
      "|         betin.co.ke|gambling|\n",
      "|          skybet.com|gambling|\n",
      "|              fdj.fr|gambling|\n",
      "|     minhngoc.net.vn|gambling|\n",
      "|   hollywoodbets.net|gambling|\n",
      "|           tipico.de|gambling|\n",
      "|       betpawa.co.ke|gambling|\n",
      "|           bet365.it|gambling|\n",
      "|          bet365.com|gambling|\n",
      "|loteriasdominican...|gambling|\n",
      "|            hkjc.com|gambling|\n",
      "|        sunmaker.com|gambling|\n",
      "|     williamhill.com|gambling|\n",
      "|       parimatch.com|gambling|\n",
      "|        freebitco.in|gambling|\n",
      "|   mismarcadores.com|gambling|\n",
      "|      flashscore.com|gambling|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cassandra-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cassandra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-74494ea9e9c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcassandra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'cassandra'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"PySpark Cassandra Test\")\\\n",
    "    .set(\"spark.cassandra.connection.host\", \"10.0.0.31:9042\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.5.1,org.apache.spark:spark-sql_2.11:2.4.5,org.apache.spark:spark-core_2.11:2.4.5 --conf spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtensions --conf spark.cassandra.connection.host=10.0.0.31:9042 pyspark-shell'\n",
    "\n",
    "sc = SparkContext(\"local\", \"movie lens app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = SQLContext(sc)\n",
    "\n",
    "cl = sql.read\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table='clients', keyspace='labdata')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------+\n",
      "|                 uid|age|gender|\n",
      "+--------------------+---+------+\n",
      "|1a4c4950-8d6a-466...| 25|     M|\n",
      "|2617c6bc-902a-4e9...| 31|     M|\n",
      "|b324d8e9-a383-445...| 24|     F|\n",
      "|4c116a98-ec5d-42a...| 32|     M|\n",
      "|f37052cd-8d0c-47c...| 20|     F|\n",
      "|bc09a17d-da60-415...| 50|     M|\n",
      "|56671ac6-ae3b-4e2...| 27|     F|\n",
      "|51546c40-7c81-43a...| 33|     F|\n",
      "|83784d72-5a66-409...| 52|     M|\n",
      "|b7003615-c4f1-4e1...| 23|     F|\n",
      "|b33d12d7-8d0d-456...| 26|     M|\n",
      "|c91635fd-7013-487...| 34|     F|\n",
      "|ad384075-c4b9-402...| 26|     M|\n",
      "|ba0837e1-db0c-49d...| 25|     M|\n",
      "|a4227cec-2692-460...| 53|     F|\n",
      "|cd10c157-9ce1-4b9...| 26|     M|\n",
      "|f1a075dc-fedd-483...| 25|     F|\n",
      "|257e475c-49eb-4e1...| 47|     M|\n",
      "|a75eef64-ffce-4d4...| 33|     F|\n",
      "|dd55d458-37fb-461...| 71|     M|\n",
      "+--------------------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json \n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.5.1,org.apache.spark:spark-sql_2.11:2.4.5,org.apache.spark:spark-core_2.11:2.4.5 --conf spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtensions --conf spark.cassandra.connection.host=10.0.0.31:9042 pyspark-shell'\n",
    "\n",
    "sparkClassPath = os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.2.12,\\\n",
    "com.datastax.spark:spark-cassandra-connector_2.11:2.5.1,\\\n",
    "org.apache.spark:spark-sql_2.11:2.4.5,\\\n",
    "org.apache.spark:spark-core_2.11:2.4.5,\\\n",
    "org.elasticsearch:elasticsearch-spark-20_2.11:6.8.9\\\n",
    "    --conf spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtensions \\\n",
    "    --conf spark.cassandra.connection.host=10.0.0.31:9042 \\\n",
    "    --num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "import json\n",
    "\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'an_en'\n",
    "password = 'QjFvQaAt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--packages org.postgresql:postgresql:42.2.12,com.datastax.spark:spark-cassandra-connector_2.11:2.5.1,org.apache.spark:spark-sql_2.11:2.4.5,org.apache.spark:spark-core_2.11:2.4.5,org.elasticsearch:elasticsearch-spark-20_2.11:6.8.9    --conf spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtensions     --conf spark.cassandra.connection.host=10.0.0.31:9042     --num-executors 2 pyspark-shell'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkClassPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"postgresql\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", sparkClassPath) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = spark.read\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table='clients', keyspace='labdata')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------+\n",
      "|                 uid|age|gender|\n",
      "+--------------------+---+------+\n",
      "|2dfee33b-b1d1-471...| 20|     F|\n",
      "|c616ee47-f9ef-4bb...| 40|     M|\n",
      "|338c083a-7e46-41d...| 27|     M|\n",
      "|8f79196c-bd82-45d...| 26|     M|\n",
      "|7de61d7b-132a-435...| 54|     F|\n",
      "|2b51d5c4-75d1-449...| 20|     M|\n",
      "|bfcf54b1-d46c-4a3...| 37|     M|\n",
      "|886c47c6-0304-405...| 40|     M|\n",
      "|f9179b42-0ff3-407...| 19|     M|\n",
      "|efba9ef1-fdaf-4cc...| 25|     F|\n",
      "|2d24f5bb-5843-48d...| 46|     F|\n",
      "|61224d2d-1eaf-432...| 21|     F|\n",
      "|4a432308-3ba3-409...| 47|     F|\n",
      "|6b478737-fd4f-445...| 32|     M|\n",
      "|290e8bd3-055e-449...| 35|     M|\n",
      "|33902272-ee0c-4f6...| 29|     M|\n",
      "|de470924-268f-437...| 51|     M|\n",
      "|39bb3a19-be87-47d...| 51|     F|\n",
      "|292db10e-8f03-4f7...| 44|     F|\n",
      "|9559e521-01be-4f4...| 52|     F|\n",
      "+--------------------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl.write\\\n",
    "#     .format(\"jdbc\")\\\n",
    "#     .option(\"url\", \"jdbc:postgresql://10.0.0.31:5432/an_en\")\\\n",
    "#     .option(\"dbtable\", \"cl\")\\\n",
    "#     .option(\"user\", username)\\\n",
    "#     .option(\"password\", password)\\\n",
    "#     .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "#     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://10.0.0.31:5432/labdata\") \\\n",
    "    .option(\"dbtable\", \"domain_cats\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|              domain|category|\n",
      "+--------------------+--------+\n",
      "|national-lottery....|gambling|\n",
      "|          rivalo.com|gambling|\n",
      "|             xoso.me|gambling|\n",
      "|         betin.co.ke|gambling|\n",
      "|          skybet.com|gambling|\n",
      "|              fdj.fr|gambling|\n",
      "|     minhngoc.net.vn|gambling|\n",
      "|   hollywoodbets.net|gambling|\n",
      "|           tipico.de|gambling|\n",
      "|       betpawa.co.ke|gambling|\n",
      "|           bet365.it|gambling|\n",
      "|          bet365.com|gambling|\n",
      "|loteriasdominican...|gambling|\n",
      "|            hkjc.com|gambling|\n",
      "|        sunmaker.com|gambling|\n",
      "|     williamhill.com|gambling|\n",
      "|       parimatch.com|gambling|\n",
      "|        freebitco.in|gambling|\n",
      "|   mismarcadores.com|gambling|\n",
      "|      flashscore.com|gambling|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = spark.read \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\n",
    "    'es.nodes', 'http://spark-data-push-adertadaltdpioy124.us-west-2.es.amazonaws.com'\n",
    ").option(\n",
    "    'es.port', 9200\n",
    ").option(\n",
    "    'es.resource', '%s/%s' % ('index_name', 'doc_type_name')\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = spark.read\\\n",
    "    .format(\"org.elasticsearch.spark.sql\")\\\n",
    "    .option(\"es.nodes\",\"10.0.0.31\")\\\n",
    "    .option(\"es.port\",\"9200\")\\\n",
    "    .option(\"user\", username) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load(\"visits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+----------+-------------------+--------------------+\n",
      "|            category|event_type|             item_id|item_price|          timestamp|                 uid|\n",
      "+--------------------+----------+--------------------+----------+-------------------+--------------------+\n",
      "|Entertainment-equ...|       buy|Entertainment-equ...|      2529|2020-01-01 11:00:00|40b29579-e845-45c...|\n",
      "|    Everyday-jewelry|       buy|  Everyday-jewelry-0|      4320|2020-01-01 11:01:00|                null|\n",
      "|             Cameras|      view|           Cameras-1|      1856|2020-01-01 11:02:00|ab0e7dd1-5899-488...|\n",
      "|             Luggage|      view|           Luggage-7|      4975|2020-01-01 11:02:00|                null|\n",
      "|       Mobile-phones|      view|    Mobile-phones-10|      3981|2020-01-01 11:03:00|d1206327-0e9f-410...|\n",
      "|    Everyday-jewelry|       buy|  Everyday-jewelry-0|      4786|2020-01-01 11:03:00|                null|\n",
      "|             Cameras|      view|          Cameras-15|       312|2020-01-01 11:03:00|6d5212d9-73eb-434...|\n",
      "|             Luggage|       buy|          Luggage-19|      2194|2020-01-01 11:04:00|                null|\n",
      "|    Sports-equipment|       buy| Sports-equipment-15|      1541|2020-01-01 11:04:00|                null|\n",
      "| Household-furniture|      view|Household-furnitu...|      3744|2020-01-01 11:05:00|                null|\n",
      "|House-repairs-pai...|      view|House-repairs-pai...|      3331|2020-01-01 11:05:00|63c58d8b-1529-4d0...|\n",
      "|             Cameras|      view|           Cameras-4|      2840|2020-01-01 11:06:00|4c514886-327a-4d5...|\n",
      "|           Computers|      view|         Computers-8|       489|2020-01-01 11:06:00|bb811c53-77c7-43a...|\n",
      "|               Shoes|      view|             Shoes-4|      3823|2020-01-01 11:06:00|9e25ab5d-a775-4af...|\n",
      "|           Computers|      view|        Computers-13|      3883|2020-01-01 11:07:00|1f39bee6-917b-4ae...|\n",
      "|       Mobile-phones|      view|    Mobile-phones-13|      4193|2020-01-01 11:08:00|cfd73ff8-2c0e-47f...|\n",
      "|Household-appliances|      view|Household-applian...|      4540|2020-01-01 11:08:00|0a54eae2-43ac-4f8...|\n",
      "|            Clothing|       buy|         Clothing-18|      4564|2020-01-01 11:08:00|b0133bf3-c9eb-461...|\n",
      "|Entertainment-equ...|      view|Entertainment-equ...|       666|2020-01-01 11:09:00|b9270841-65ed-432...|\n",
      "|    Sports-equipment|      view| Sports-equipment-13|      1218|2020-01-01 11:09:00|                null|\n",
      "+--------------------+----------+--------------------+----------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
